{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "xVTz4Nz8kuNi",
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "<h2> A Toy Problem <\/h2>\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "plt.style.use('ggplot')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "plt.rcParams[\"figure.figsize\"] = (15, 8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 51
                },
                "colab_type": "code",
                "collapsed": true,
                "id": "dLlrlFUeRewl",
                "lines_to_next_cell": 2,
                "outputId": "8b1b0ced-f70a-4cfa-cb4b-041476731885"
            },
            "outputs": [],
            "source": [
                "# random manual seed for consistency\n",
                "# read more at https:\/\/pytorch.org\/docs\/stable\/notes\/randomness.html\n",
                "\n",
                "torch.manual_seed(0)\n",
                "\n",
                "num_data = 3\n",
                "\n",
                "# Input \n",
                "x = 10 * torch.rand(num_data)\n",
                "print(x)\n",
                "\n",
                "# Output\n",
                "y = x + torch.randn_like(x)\n",
                "print(y)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "5ZfWqcXNlJgT",
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "The goal is to predict $y$ given some value of $x$. To do this we will fit a line that goes through the data points $(x_i, y_i)$. We will simplify the problem so that the line passes through the origin. The equation for such a line is \n",
                "\n",
                "$$\n",
                "y = mx\n",
                "$$\n",
                "\n",
                "We have a set of data points $(x_i, y_i)$, and they should all satisfy the equation above. Therefore, \n",
                "\n",
                "$$\n",
                "y_i = m x_i\n",
                "$$\n",
                "\n",
                "Unless we have perfect data with no noise, even the best $m$ we can find will not perfectly fit the data. So, we will have an **error** or a **residual** given by\n",
                "\n",
                "$$\n",
                "e_i = (y_i - m x_i) \n",
                "$$\n",
                "\n",
                "We want to find a value of $m$ that minimizes the error above. Positive or negative values of error are equally bad for us. So, we are interested in mimimizing the square of the error above. In addition, we want to mimimize the squared error over all the data points.  \n",
                "\n",
                "In other words, we want to mimize a function of the residual that takes the following form\n",
                "\n",
                "$$\n",
                "l = \\sum^n_{i=1}(y_i - m x_i)^2 \\\\\n",
                "$$\n",
                "\n",
                "This function is called the **loss function**. The sum of squared errors is just one type of loss function. There are other types of loss functions which will learn about later in the course. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "gT-jBx9QZLO1",
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "<h2>Brute Force Search<\/h2>\n",
                "\n",
                "A naive way to find the value of $m$ is to do a brute force search over a large range of $m$. Let's just do it for fun. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 313
                },
                "colab_type": "code",
                "collapsed": true,
                "id": "wNr_9ekiR2JG",
                "outputId": "22ece6d6-b7ce-45b5-ebd7-124114ebf34d"
            },
            "outputs": [],
            "source": [
                "# Linear model \n",
                "# y = mx \n",
                "\n",
                "# Minimum value of m\n",
                "min_val = 0.0 \n",
                "\n",
                "# Maximum value of m\n",
                "max_val = 2.0 \n",
                "\n",
                "# Number of steps between min and max values\n",
                "num_steps = 10\n",
                "\n",
                "# Step size\n",
                "step_size = (max_val - min_val)\/(num_steps - 1)\n",
                "\n",
                "# Space for storing all values of m\n",
                "m = torch.zeros(num_steps)\n",
                "\n",
                "# Space for storing loss corresponding \n",
                "# to different values of m. \n",
                "loss = torch.zeros(num_steps)\n",
                "\n",
                "# Calculate loss for all possible m\n",
                "for i in range(0, num_steps):\n",
                "    m[i] = min_val +  i * step_size\n",
                "    e = y - m[i] * x\n",
                "    loss[i] = torch.sum(torch.mul(e,e)) \n",
                "\n",
                "\n",
                "# Find the index for lowest loss\n",
                "i = torch.argmin(loss)\n",
                "\n",
                "# Minimum loss\n",
                "print('Minimum Loss : ', loss[i])\n",
                "\n",
                "# Find the value of m corresponding to lowest loss\n",
                "print('Best parameter : ', m[i])\n",
                "\n",
                "# Plot loss vs m  \n",
                "plt.figure\n",
                "plt.plot(m.numpy(), loss.numpy())\n",
                "plt.xlabel('m')\n",
                "plt.ylabel('loss')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "kkMDy4x3ZXek",
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "<h2>Gradient Descent<\/h2>\n",
                "\n",
                "The above plot shows that we can start from an initial guess of $m$, follow the slope of the curve, and keep iterating to reach the bottom of the curve. This simple idea is called gradient descent. \n",
                "\n",
                "Now the question is how do we calculate the gradient of the loss function with respect to $m$ at any point. \n",
                "\n",
                "This simply requires high school calculus. \n",
                "\n",
                "$$\n",
                "\\begin{align}\n",
                "l &= \\sum^n_{i=1}(y_i - m x_i)^2 \\\\\n",
                "\\frac{\\partial l}{\\partial m}  &= -2 \\sum^n_{i=1} x_i(y_i - m x_i) \\\\\n",
                "\\end{align}\n",
                "$$\n",
                "\n",
                "To follow the slope of the curve, we need to move m in the direction of negative gradient. However, we need to control the rate at which we go down the slope so that we do not overshoot the minimum. So we use a parameter $\\lambda$ called the learning rate. \n",
                "\n",
                "$$\n",
                "m_k = m_{k-1} - \\lambda \\frac{\\partial l}{\\partial m}\n",
                "$$\n",
                "\n",
                "That is it! \n",
                "\n",
                "Let's implement this in code to see that it really works. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 313
                },
                "colab_type": "code",
                "collapsed": true,
                "id": "KLrS-boWiqjJ",
                "outputId": "6ffb5ae5-9bcf-4d24-c97c-ce28acf43dd6"
            },
            "outputs": [],
            "source": [
                "num_iter = 10\n",
                "\n",
                "lr = 0.01\n",
                "m = 2 \n",
                "\n",
                "loss = torch.zeros(num_iter)\n",
                "# Calculate loss for all possible m\n",
                "for i in range(0, num_iter):\n",
                "    g = -2 * torch.sum(x * (y - m * x))\n",
                "    m = m -  lr * g\n",
                "    e = y - m * x\n",
                "    loss[i] = torch.sum(torch.mul(e,e)) \n",
                "    \n",
                "print('Minimum loss : ', loss[-1])\n",
                "print('Best parameter : ', m)\n",
                "\n",
                "# Plot loss vs m  \n",
                "plt.figure\n",
                "plt.plot(loss.numpy())\n",
                "plt.ylabel('loss')\n",
                "plt.xlabel('iterations')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "rQB6vopn1WqL",
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "<h2>Stochastic Gradient Descent<\/h2>\n",
                "\n",
                "In this example, we have only three data points. In the real world, we can have millions of examples. Calculating gradient based on all data points can be computationally expensive. Fortunately, it is not necessary to use all data points for computing the gradient. \n",
                "\n",
                "We can use a **single** randomly chosen data point to compute the gradient at each step. Even though the gradient at each step is not as accurate, the idea still works as you can see in this example."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 330
                },
                "colab_type": "code",
                "collapsed": true,
                "id": "A24ZcpYF1Voj",
                "nbgrader": {
                    "grade": false,
                    "locked": false,
                    "solution": false
                },
                "outputId": "4861f464-63a0-45f3-a6ac-e088b7931493"
            },
            "outputs": [],
            "source": [
                "num_iter = 20\n",
                "\n",
                "lr = 0.01\n",
                "m = 2 \n",
                "print()\n",
                "loss = torch.zeros(num_iter)\n",
                "\n",
                "for i in range(0, num_iter):\n",
                "\n",
                "    # Randomly select a training data point\n",
                "    k = torch.randint(0, len(y), (1,))[0]\n",
                "\n",
                "    # Calculate gradient using a single data point\n",
                "    g = -2 * x[k] * (y[k] - m * x[k])\n",
                "    m = m -  lr * g\n",
                "    e = y - m * x\n",
                "    loss[i] = torch.sum(torch.mul(e,e)) \n",
                "\n",
                "print('Minimum loss : ', loss[-1])\n",
                "print('Best parameter : ', m)\n",
                "\n",
                "# Plot loss vs m  \n",
                "plt.figure\n",
                "plt.plot(loss.numpy())\n",
                "plt.ylabel('loss')\n",
                "plt.xlabel('iterations')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "4edYK9miKr0T",
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "<h2>Stochastic Gradient Descent with Minibatch<\/h2>\n",
                "\n",
                "In the previous section, we saw that it is possible to calculate gradient based on a single data point, and as long as we were running many iterations, Stochastic Gradient Descent will still work. \n",
                "\n",
                "However, using more than one data point for gradient calculation has two advantages\n",
                "1. Gradients are less noisy. \n",
                "2. GPUs can process multiple data points ( or images ) in one shot. \n",
                "\n",
                "So, we get better results and faster convergence if we use a small batch of data points, called a **mini-batch**, to compute the gradients. \n",
                "\n",
                "Let's implement this in code, and see for ourselves. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 330
                },
                "colab_type": "code",
                "collapsed": true,
                "id": "OnBE84ym1wby",
                "lines_to_next_cell": 2,
                "outputId": "2a6eb9dc-ac22-4d61-af15-375e7b4f57ca"
            },
            "outputs": [],
            "source": [
                "num_data = 1000\n",
                "batch_size = 10\n",
                "\n",
                "# Input \n",
                "x = 10 * torch.rand(num_data)\n",
                "\n",
                "# Output\n",
                "y = x + torch.randn_like(x)\n",
                "\n",
                "num_iter = 30\n",
                "\n",
                "lr = 0.001\n",
                "m = 2 \n",
                "print()\n",
                "loss = torch.zeros(num_iter)\n",
                "\n",
                "for i in range(0, num_iter):\n",
                "\n",
                "    # Randomly select a training data point\n",
                "    k = torch.randint(0, len(y)-1, (batch_size,))\n",
                "  \n",
                "    # Calculate gradient using a mini-batch\n",
                "    g = -2 * torch.sum(x[k] * (y[k] - m * x[k]))\n",
                "    m = m -  lr * g\n",
                "    e = y - m * x\n",
                "    loss[i] = torch.sum(torch.mul(e,e)) \n",
                "\n",
                "print('Minimum loss : ', loss[-1])\n",
                "print('Best parameter : ', m)\n",
                "\n",
                "# Plot loss vs m  \n",
                "plt.figure\n",
                "plt.plot(loss.numpy())\n",
                "plt.ylabel('loss')\n",
                "plt.xlabel('iterations')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab": [],
                "colab_type": "code",
                "id": "FY1gq0SRMnkS",
                "lines_to_next_cell": 0,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "# <font style=\"color:blue\">Exercise<\/font>\n",
                "Here are a few things that you may try with the above code and have fun!\n",
                "\n",
                "1. Change the learning rate parameter and plot the curves. ( make it higher and lower )\n",
                "2. When learning rate is set to lower values, do you need to train for longer iterations or less?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "lines_to_next_cell": 2
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python37"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}